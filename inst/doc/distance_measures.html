<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Dr. Simon Müller" />

<meta name="date" content="2023-04-29" />

<title>Motivation</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>







<style type="text/css">body {
margin: 0 auto;
background-color: white;

/ font-family:Georgia, Palatino, serif;
font-family: "Open Sans", "Book Antiqua", Palatino, serif;
/ font-family:Arial, Helvetica, sans-serif;
/ font-family:Tahoma, Verdana, Geneva, sans-serif;
/ font-family:Courier, monospace;
/ font-family:"Times New Roman", Times, serif;
	color: #333333; 
/ color: #000000; 
/ color: #666666; 	/ color: #E3E3E3; 
/ color: white; line-height: 100%;
max-width: 800px;
padding: 10px;
font-size: 17px;
text-align: justify;
text-justify: inter-word;
}
p {
line-height: 150%;
/ max-width: 540px;
max-width: 960px;
margin-bottom: 5px;
font-weight: 400; / color: #333333
}
h1, h2, h3, h4, h5, h6 {
font-weight: 400;
margin-top: 35px;
margin-bottom: 15px;
padding-top: 10px;
}
h1 {
margin-top: 70px;
color: #2c3e50;
font-size:230%;
font-variant:small-caps;
padding-bottom:20px;
width:100%;
border-bottom:1px solid #2c3e50;
}
h2 {
font-size:160%;
}
h3 {
font-size:130%;
}
h4 {
font-size:120%;
font-variant:small-caps;
}
h5 {
font-size:120%;
}
h6 {
font-size:120%;
font-variant:small-caps;
}
a {
color: #2c3e50;
margin: 0;
padding: 0;
vertical-align: baseline;
}
a:hover {
text-decoration: blink;
color: green;
}
a:visited {
color: gray;
}
ul, ol {
padding: 0;
margin: 0px 0px 0px 50px;
}
ul {
list-style-type: square;
list-style-position: inside;
}
li {
line-height:150% }
li ul, li ul {
margin-left: 24px;
}
pre {
padding: 0px 10px;
max-width: 800px;
white-space: pre-wrap;
}
code {
font-family: Consolas, Monaco, Andale Mono, monospace, courrier new;
line-height: 1.5;
font-size: 15px;
background: #F8F8F8;
border-radius: 4px;
padding: 5px;
display: inline-block;
max-width: 800px;
white-space: pre-wrap;
}
li code, p code {
background: #f0f0f0;
color: #2c3e50;
padding: 0px 5px 0px 5px;
}
code.r, code.cpp {
display: block;
word-wrap: break-word;
border: 1px solid #2c3e50; }
aside {
display: block;
float: right;
width: 390px;
}
blockquote {
border-left:.5em solid #2c3e50;
background: #F8F8F8;
padding: 0em 1em 0em 1em;
margin-left:10px;
max-width: 500px;
}
blockquote cite {
line-height:10px;
color:#bfbfbf;
}
blockquote cite:before {
/content: '\2014 \00A0';
}
blockquote p, blockquote li { color: #666;
}
hr {
/ width: 540px;
text-align: left;
margin: 0 auto 0 0;
color: #999;
}

table {
width: 100%;
border-top: 1px solid #919699;
border-left: 1px solid #919699;
border-spacing: 0;
}
table th {
padding: 4px 8px 4px 8px;
text-align: center;
color: white;
background: #2c3e50;
border-bottom: 1px solid #919699;
border-right: 1px solid #919699;
}
table th p {
font-weight: bold;
margin-bottom: 0px; }
table td {
padding: 8px;	vertical-align: top;
border-bottom: 1px solid #919699;
border-right: 1px solid #919699;
}
table td:last-child {
/background: lightgray;
text-align: right;
}
table td p {
margin-bottom: 0px; }
table td p + p {
margin-top: 5px; }
table td p + p + p {
margin-top: 5px; }</style>




</head>

<body>




<h1 class="title toc-ignore">Motivation</h1>
<h4 class="author">Dr. Simon Müller</h4>
<h4 class="date">2023-04-29</h4>


<div id="TOC">
<ul>
<li><a href="#objective" id="toc-objective"><span class="toc-section-number">1</span> Objective</a>
<ul>
<li><a href="#define-a-distance-measure-for-numerical-and-categorical-features" id="toc-define-a-distance-measure-for-numerical-and-categorical-features"><span class="toc-section-number">1.0.1</span> Define a Distance Measure for
Numerical and Categorical Features</a></li>
<li><a href="#weighted-distance-measure" id="toc-weighted-distance-measure"><span class="toc-section-number">1.1</span> Weighted Distance Measure</a></li>
</ul></li>
<li><a href="#statistical-model" id="toc-statistical-model"><span class="toc-section-number">2</span> Statistical Model</a></li>
<li><a href="#random-forests" id="toc-random-forests"><span class="toc-section-number">3</span> Random Forests</a>
<ul>
<li><a href="#proximity-measure" id="toc-proximity-measure"><span class="toc-section-number">3.1</span> Proximity Measure</a></li>
<li><a href="#depth-measure-a-modified-proximity-measure" id="toc-depth-measure-a-modified-proximity-measure"><span class="toc-section-number">3.2</span> Depth Measure: A modified
proximity measure</a></li>
</ul></li>
</ul>
</div>

<div id="objective" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Objective</h1>
<div id="define-a-distance-measure-for-numerical-and-categorical-features" class="section level3" number="1.0.1">
<h3><span class="header-section-number">1.0.1</span> Define a Distance
Measure for Numerical and Categorical Features</h3>
<p>Let <span class="math inline">\(x_i =
\left(x_i^1,\cdots,x_i^p\right)^T\)</span> and <span class="math inline">\(x_j = \left(x_j^1,\cdots,x_j^p\right)^T\)</span>
be the feature vectors of tow distinct patients <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>. A first rough idea may be to calculate
the <span class="math inline">\(L_1-\)</span>norm of this two feature
vectors: <span class="math display">\[
\|x_i - x_j\|_{L_1}=\frac{1}{p}\sum\limits_{k=1}^p|x_i^k - x_j^k|
\]</span></p>
<p>However, this naive approach arises from some problems:</p>
<ol style="list-style-type: decimal">
<li><p>This measure is well-defined for numerical features but not for
categorical features, such as gender (male/female). We need a method to
define the distance for categorical variables.</p></li>
<li><p><span class="math inline">\(|x_i^k - x_j^k|\)</span> is not
scale-invariant, meaning that if one changes the unit of measurement
(e.g., from meters to centimeters), the contribution of this feature
would increase by a factor of 100. Features with large values will
dominate the distance.</p></li>
<li><p>All features are treated equally, which may not reflect their
actual importance. For example, in the case of lung cancer, the
patient’s smoking status (no/yes) seems more relevant than the city they
live in.</p></li>
</ol>
<p>To address these issues, we propose a machine learning approach that
can handle both numerical and categorical features while accounting for
their varying importance:</p>
<ol style="list-style-type: decimal">
<li>For categorical features, use a distance metric such as the Gower
distance, which can handle mixed data types. This metric assigns a
distance of 0 for matching categories and 1 for non-matching
categories.</li>
<li>Normalize numerical features to make them scale-invariant. This can
be done using min-max scaling or standardization (subtracting the mean
and dividing by the standard deviation).</li>
<li>Assign weights to features based on their importance in predicting
the outcome. This can be achieved using feature selection techniques or
by incorporating feature importance scores from machine learning models
such as decision trees or random forests.</li>
</ol>
<p>By employing a machine learning approach to define the distance
measure, we can effectively address the challenges associated with
numerical and categorical features while accounting for their relative
importance in the context of the problem.</p>
</div>
<div id="weighted-distance-measure" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Weighted Distance
Measure</h2>
<p>To address the challenges discussed earlier, we need to perform two
steps:</p>
<ol style="list-style-type: decimal">
<li><p>Generalize the L1-distance to handle different variable types,
including numerical and categorical features.</p></li>
<li><p>Assign appropriate weights to each feature to:</p>
<p>a. eliminate the dependency on the scale of the variables,</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>account for varying importance across features, and</li>
<li>consider the impact size of each feature.</li>
</ol></li>
</ol>
<p>By incorporating these modifications, we arrive at the following
weighted distance measure:</p>
<p><span class="math display">\[
d(x_i, x_j) = \sum\limits_{k=1}^p |\alpha(x_i^k, x_j^k) d(x_i^k, x_j^k)|
\]</span></p>
<p>The weighted distance measure now includes weights <span class="math inline">\(\alpha(x_i^k, x_j^k)\)</span>, which are
determined based on the training data. In the next section, we will
discuss how to obtain these weights and effectively compute the weighted
distance measure for mixed data types and varying feature
importance.</p>
</div>
</div>
<div id="statistical-model" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Statistical Model</h1>
<p>Let <span class="math inline">\(\alpha(x_i^k, x_j^k)\)</span> the
weights and <span class="math inline">\(d(x_i^k, x_j^k)\)</span> the
distance for feature <span class="math inline">\(k\)</span> and
observation <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>. We will define them as:</p>
<p>If feature <span class="math inline">\(k\)</span> is numerical,
then</p>
<p><span class="math display">\[d(x_i^k, x_j^k) = x_i^k - x_j^k\]</span>
and</p>
<p><span class="math display">\[\alpha(x_i^k, x_j^k) =
\hat{\beta_k}\]</span></p>
<p>If feature <span class="math inline">\(k\)</span> is categorical,
then</p>
<p><span class="math display">\[d(x_i^k, x_j^k) = 1 \text{ when } x_i^k
= x_j^k \text{ else } 0\]</span> and <span class="math display">\[\alpha(x_i^k, x_j^k) = \hat{\beta_k}^i -
\hat{\beta_k}^j,\]</span></p>
<p>where <span class="math inline">\(\hat{\beta_k}^k\)</span> are the
coefficients of a regression model (linear, logistic, or CPH model).By
defining the distance and weight measures in this way, we account for
both numerical and categorical features while incorporating the relative
importance of each feature in the model. The weights are determined
based on the regression model’s coefficients, reflecting the impact of
each feature on the prediction. This approach provides a more meaningful
and interpretable distance measure for mixed data types and varying
feature importance.</p>
</div>
<div id="random-forests" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Random Forests</h1>
<div id="proximity-measure" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Proximity
Measure</h2>
<p>Two observations <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are considered more similar when the
fraction of trees in which patient <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> share the same terminal node is close
to one (Breiman, 2002).</p>
<p><span class="math display">\[d(x_i, x_j)^2 = 1 -
\frac{1}{M}\sum\limits_{t=1}^T 1_{[x_i \text{ and } x_j \text{ share the
same terminal node in tree } t]},\]</span> where <span class="math inline">\(M\)</span> is the number of trees that contain
both observations and <span class="math inline">\(T\)</span> is the
total number of trees.A drawback of this measure is that the decision is
binary, meaning that potentially similar observations might be counted
as dissimilar. For example, suppose a final cut-off is consistently made
around age 58, and observation 1 has an age of 56 while observation 2
has an age of 60. In this case, the distance between observation 1 and
observation 2 would be the same as the distance between observation 1
and an observation with an age of 80. This limitation makes the
proximity measure less sensitive to small differences between
observations, potentially affecting the overall analysis of
similarity.</p>
</div>
<div id="depth-measure-a-modified-proximity-measure" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Depth Measure: A
modified proximity measure</h2>
<p>In contrast to the proximity measure, the depth measure takes into
account the number of edges between two observations instead of their
final nodes in each tree. This distance measure is averaged over all
trees and is defined as:</p>
<p><span class="math display">\[
d(x_i, x_j) = \frac{1}{M}\sum\limits_{t=1}^T g_{ij},
\]</span></p>
<p>where <span class="math inline">\(M\)</span> is the number of trees
containing both observations, and <span class="math inline">\(g_{ij}\)</span> is the number of edges between the
end nodes of observation <span class="math inline">\(i\)</span> and
<span class="math inline">\(j\)</span> in tree <span class="math inline">\(t\)</span>. This measure considers the structure
of the trees and provides a more nuanced understanding of the similarity
between observations.</p>
<p>For more details and a thorough explanation of the depth measure,
refer to the publication by Englund and Verikas: “<a href="https://www.sciencedirect.com/science/article/abs/pii/S095741741200810X">A
novel approach to estimate proximity in a random forest: An exploratory
study.</a>” The depth measure addresses some of the limitations of the
proximity measure by considering the tree structure and the path
traversed by the observations, which results in a more accurate and
informative distance measure.</p>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
